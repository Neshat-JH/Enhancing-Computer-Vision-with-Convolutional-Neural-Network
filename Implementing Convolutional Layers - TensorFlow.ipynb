{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving Computer Vision Accuracy using Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Image Detection without using Convolutions:\n",
    "Let's start by making a simple Neural Network using Tensorflow and keras. There is a whole Jupyter notebook with more details about this, so feel free to check it out!\n",
    "Later we are going to use the accuracy, loss and run-time of our simple model to compare with our Neural Network with convolutions implemented! So stay tuned.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.5000 - accuracy: 0.8253\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3750 - accuracy: 0.8662\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3374 - accuracy: 0.8774\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3125 - accuracy: 0.8861\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2951 - accuracy: 0.8917\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3460 - accuracy: 0.8758\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#Accessing data\n",
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "#Scaling the input sets\n",
    "training_images=training_images / 255.0\n",
    "test_images=test_images / 255.0\n",
    "\n",
    "#Creating the model\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(training_images, training_labels, epochs=5)\n",
    "\n",
    "#Evaluating the model\n",
    "test_loss = model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our training model accuracy is 89%, and we got around 87% for our testing image set. Although these are some good results let's see how using convolutions can improve each aspect!(or maybe not!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Image Detection using Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to start from scratch!\n",
    "I will import tensorflow, then will get my data from fashion_mnist.\n",
    "Then I have to reshape my data!If you don't do this, you'll get an error when training as the Convolutions do not recognize the shape.\"That's because the first convolution expects a single tensor containing everything, so instead of 60,000 28x28x1 items in a list, we have a single 4D list that is 60,000x28x28x1, and the same for the test images.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensotFlow Version: 2.2.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensotFlow Version:\", tf.__version__)\n",
    "\n",
    "#Getting the data\n",
    "mnist=tf.keras.datasets.fashion_mnist\n",
    "(training_image, training_label), (testing_image, testing_label)=mnist.load_data()\n",
    "\n",
    "#Reshaping input data\n",
    "training_image=training_image/255\n",
    "testing_image=testing_image/255\n",
    "\n",
    "training_image= training_image.reshape(60000, 28, 28, 1)\n",
    "testing_image= testing_image.reshape(10000, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to create the model. Instead of starting with the input layer, here we're going to start with a convolution! Then we'll follow each by a pooling layer (to compress the image for the next layer!). For example, the convolution looks something like:\n",
    "\n",
    "*tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1))*\n",
    "\n",
    "* So 32 is the arbitrary number of your filters! 32 is a good number to start.\n",
    "* (3, 3) is the size of the convolution. Here is a 3 X 3 grid.\n",
    "* The activation function here is 'relu' which basically returns a positive output.\n",
    "* Last but not least is the shape of the input image; which here is 28 X 28 pixels.\n",
    "\n",
    "After each convolution comes a pooling:\n",
    "\n",
    "*tf.keras.layers.MaxPooling2D((2,2))*\n",
    "\n",
    "* By choosing (2,2) we are reducing each image by 1/4 (25%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'), #adding another convolution followed by pooling\n",
    "  tf.keras.layers.MaxPooling2D(2,2),\n",
    "  tf.keras.layers.Flatten(), #flattening the output. from here is the same simple NN\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiling out model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 59s 31ms/step - loss: 0.4365 - accuracy: 0.84130s - l\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 58s 31ms/step - loss: 0.2920 - accuracy: 0.8932\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 59s 31ms/step - loss: 0.2475 - accuracy: 0.90840s - loss: 0.247\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 58s 31ms/step - loss: 0.2150 - accuracy: 0.9205\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 57s 30ms/step - loss: 0.1889 - accuracy: 0.9305\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.2618 - accuracy: 0.9060\n",
      "0.906000018119812\n"
     ]
    }
   ],
   "source": [
    "#Calling the fit method to do the training\n",
    "model.fit(training_image, training_label, epochs=5)\n",
    "\n",
    "#Evaluating the model\n",
    "test_loss, test_acc = model.evaluate(testing_image, testing_label)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the training took longer, the accuracy of the training model increased to 92% and 91% when using our testing set!\n",
    "We can improve the accuracy by changing the number of epoch however there are always chances of overfitting with higher epochs which we need to keep in mind!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Implementing a \"callback\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am basically going to repeat the previous NN using convolution using the mnist dataset. I will only use 32 filters, and will set the epoch to 10! However I will use a callback function so that the training stops when I get to 99% accuracy! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.1440 - accuracy: 0.9567 - ETA: 0s - loss: 0.1444 - accuracy\n",
      "Reached 99% accuracy so cancelling training!\n",
      "1875/1875 [==============================] - 31s 17ms/step - loss: 0.1440 - accuracy: 0.9567\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.0733 - accuracy: 0.9765\n",
      "0.9764999747276306\n"
     ]
    }
   ],
   "source": [
    "#Getting our data together!\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "training_images=training_images.reshape(60000, 28, 28, 1)\n",
    "training_images=training_images / 255.0\n",
    "\n",
    "test_images = test_images.reshape(10000, 28, 28, 1)\n",
    "test_images=test_images/255.0\n",
    "\n",
    "#Callback function\n",
    "class MyCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if(logs.get('accuracy')<=0.99):\n",
    "            print(\"\\nReached 99% accuracy so cancelling training!\")\n",
    "            self.model.stop_training=True\n",
    "            \n",
    "callbacks=MyCallback()\n",
    "\n",
    "\n",
    "#Making the model\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(training_images, training_labels, epochs=10, callbacks=[callbacks])\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the epoch when the accuracy reached 99% the training was stopped! So we didn't have to wait till all 10 epochs are over!\n",
    "\n",
    "Hope you enjoyed this project! I really recommend taking the Coursera \"Introduction to TensorFlow for Artificial Intelligence, Machine Learning, and Deep Learning\". Laurance Moroney does a great job explaining ins and outs of each neural network!\n",
    "\n",
    "Cheers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
